# -*- coding: utf-8 -*-
"""aivengers-project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11KaZU_2XCkjgrfS5I86UPCVutQNfmMHt
"""

#we have accomodations to extend this assignment

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import time
#https://github.com/SamTlor/AI_proj2/tree/main

#declaring norm function with the 2d array dataset
def norm(dataset):

    #looping though each column index except the last colums
    for column_index in range(dataset.shape[1] - 1):

        #determining min and max values from the current columns to scale the data between [0,1]
        max_val = dataset[column_index].max()
        min_val = dataset[column_index].min()

        #normalizing the column to fall within the range [0,1]
        #this will modify the dataset in place allowing the original values to be replaced by the normalized values
        dataset[column_index] = (dataset[column_index] - min_val) / (max_val - min_val)

def train(dataset, split, error_threshold, alpha):

  # Plotting training data
    plt.scatter(training[training.iloc[:,2]==0].iloc[:,0], training[training.iloc[:,2]==0].iloc[:,1], label='Class 0 - Training', alpha=0.6)
    plt.scatter(training[training.iloc[:,2]==1].iloc[:,0], training[training.iloc[:,2]==1].iloc[:,1], label='Class 1 - Training', alpha=0.6)

    # Plotting testing data
    plt.scatter(testing[testing.iloc[:,2]==0].iloc[:,0], testing[testing.iloc[:,2]==0].iloc[:,1], label='Class 0 - Testing', alpha=0.6, marker='x')
    plt.scatter(testing[testing.iloc[:,2]==1].iloc[:,0], testing[testing.iloc[:,2]==1].iloc[:,1], label='Class 1 - Testing', alpha=0.6, marker='x')

    # Plotting
    plt.figure()
    plt.scatter(training.iloc[:, 0], training.iloc[:, 1], c=training.iloc[:, 2], cmap='viridis', label='Training Data')
    plt.scatter(testing.iloc[:, 0], testing.iloc[:, 1], c=testing.iloc[:, 2], cmap='viridis', marker='x', label='Testing Data')

    # Plotting decision boundary
    x = np.linspace(start=min(dataset.iloc[:, 0]), stop=max(dataset.iloc[:, 0]), num=100)
    y = (-w[0] * x - w[2]) / w[1]
    plt.plot(x, y, '-r', label='Decision Boundary')


    plt.legend()
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Data and Decision Boundary')
    plt.grid(True)
    plt.show()
    return w

#predict function with 2 parmeters weight vector (w) and input vector (x)
def predict(w, x):

  #computing the dot product between weight and input vectors
    scaled_x = np.dot(w, x)

    if scaled_x > 0:
      #the result of the dot product
        return 1
    else:
        return 0

# get accuracy, confusion matrices and rates and return the optimized weight vector
def train(dataset, split, error_threshold, alpha):
#dataset-data the model is trained on
#split-fraction of the dataset that is being used for training
#error-the fucntion tha will keep updating the weights until the error becomes less than this threshold
#alpha-the learning rate for the perceptron training

    #splitting into training and testing data
    group0 = dataset[dataset.iloc[:, 2] == 0]
    group1 = dataset[dataset.iloc[:, 2] == 1]
    sample_size = int(min(len(group0), len(group1)) * split)
    sampled_subset0 = group0.sample(n = sample_size, random_state = 42)
    sampled_subset1 = group1.sample(n = sample_size, random_state = 42)
    training = pd.concat([sampled_subset0, sampled_subset1])
    testing = dataset.drop(training.index)



    #initialization stuff
    limit = 5000
    patterns = training.shape[0]
    w = [random.uniform(-0.5, 0.5) for _ in range(training.shape[1])]



    #training
    i = 0
    error = patterns
    #while i is less than 5000 or it is not accurate enough
    while i < limit and error > error_threshold:
        #the error in this iteration of the neuron
        error = 0

        for row in range(patterns):
            x = training.iloc[row].values
            scaled_x = predict(w, x)

            #for total error sum[(out - desired)^2]. (-1)^2 = 1 and 1^2 = 1 so += 1 when they're different
            if x[2] != scaled_x:
                error += 1

            delta_weight = alpha * (x[2] - scaled_x)
            delta_weighted_x = x * delta_weight
            w = np.array(w) + np.array(delta_weighted_x)
        i += 1

    print(f"Total error in training is: {error}")



    #testing
    true_positive = false_positive = true_negative = false_negative = 0
    for row in range(testing.shape[0]):
        x = testing.iloc[row].values
        predicted = predict(w, x)
        true = x[2]

        if predicted == 1 and true == 1:
            true_positive += 1
        elif predicted == 1 and true == 0:
            false_positive += 1
        elif predicted == 0 and true == 1:
            false_negative += 1
        else:
            true_negative += 1

    accuracy = (true_positive + true_negative) / testing.shape[0]
    true_positive_rate = true_positive / (true_positive + false_positive)
    false_positive_rate = false_positive / (true_positive + false_positive)
    true_negative_rate = true_negative / (true_negative + false_negative)
    false_negative_rate = false_negative / (true_negative + false_negative)

    print(f"accuracy: {accuracy}")
    print("\t\tTrue yes\t\tTrue no")
    print(f"Predicted yes\t{true_positive} ({true_positive_rate}%)\t\t{false_positive} ({false_positive_rate}%)")
    print(f"Predicted no\t{false_negative} ({false_negative_rate}%)\t\t{true_negative} ({true_negative_rate}%)")




    #[training or testing idk the right one goes here].plot(x = [whatever].iloc[:, 0], y = [whatever].iloc[:, 1])
    #line = needs to be on graph too
    # w[0] * y + w[1] * x + z = 0
    #y and x might be switched idk figure it out


    return w

#read the datasets
a = pd.read_csv("groupA.csv", header = None)
b = pd.read_csv("groupB.csv", header = None)
c = pd.read_csv("groupC.csv", header = None)



#convert to numeric
for col in a.columns:
    a[col] = a[col].apply(pd.to_numeric, errors = 'coerce')
    b[col] = b[col].apply(pd.to_numeric, errors = 'coerce')
    c[col] = c[col].apply(pd.to_numeric, errors = 'coerce')



#normalize the data
norm(a)
norm(b)
norm(c)

#matplotlib graphs
plt.figure()

  # Plotting training data
plt.scatter(training[training.iloc[:,2]==0].iloc[:,0], training[training.iloc[:,2]==0].iloc[:,1], label='Class 0 - Training', alpha=0.6)
plt.scatter(training[training.iloc[:,2]==1].iloc[:,0], training[training.iloc[:,2]==1].iloc[:,1], label='Class 1 - Training', alpha=0.6)

# Plotting testing data
plt.scatter(testing[testing.iloc[:,2]==0].iloc[:,0], testing[testing.iloc[:,2]==0].iloc[:,1], label='Class 0 - Testing', alpha=0.6, marker='x')
plt.scatter(testing[testing.iloc[:,2]==1].iloc[:,0], testing[testing.iloc[:,2]==1].iloc[:,1], label='Class 1 - Testing', alpha=0.6, marker='x')

# Plotting decision boundary
# For a 2D dataset, the decision boundary is a line: ax + by + c = 0 => y = (-c - ax) / b
x = np.linspace(training.iloc[:,0].min(), training.iloc[:,0].max(), 400)
y = (-w[2] - w[0]*x) / w[1]
plt.plot(x, y, label='Decision Boundary')

plt.legend()
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Data and Decision Boundary')
plt.grid(True)
plt.show()



#hard activation
print("HARD ACTIVATION")
print("---------------------------------------")
print("\nGroupA")
print(train(a, 3/4, 0.00001, 0.1))
print(train(a, 1/4, 0.00001, 0.1))

print("\nGroupB")
print(train(b, 3/4, 40, 0.1))
print(train(b, 1/4, 40, 0.1))

print("\nGroupC")
print(train(c, 3/4, 700, 0.1)) #fix the train method
print(train(c, 1/4, 700, 0.1))


#soft activation
print("SOFT ACTIVATION")
print("---------------------------------------")
print("\nGroupA")
print(train(a, 3/4, 0.00001, 0.01)) #75
print(train(a, 1/4, 0.00001, 0.01)) #25

print("\nGroupB")
print(train(b, 3/4, 40, 0.01))#75
print(train(b, 1/4, 40, 0.01))#25

print("\nGroupC")
print(train(c, 3/4, 700, 0.01))#75
print(train(c, 1/4, 700, 0.01))#25