{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/ZachM/Projects/AI_proj2/proj2.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ZachM/Projects/AI_proj2/proj2.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ZachM/Projects/AI_proj2/proj2.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ZachM/Projects/AI_proj2/proj2.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m shuffle\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#we have accomodations to extend this assignment\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring norm function with the 2d array dataset\n",
    "def norm(dataset):\n",
    "    #looping though each column index except the last colums\n",
    "    for column_index in range(dataset.shape[1] - 1):\n",
    "        \n",
    "        #determining min and max values from the current columns to scale the data between [0,1]\n",
    "        max_val = dataset[column_index].max()\n",
    "        min_val = dataset[column_index].min()\n",
    "\n",
    "        #normalizing the column to fall within the range [0,1]\n",
    "        #this will modify the dataset in place allowing the original values to be replaced by the normalized values\n",
    "        dataset.iloc[:, column_index] = (dataset.iloc[:, column_index] - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict function with 2 parmeters weight vector (w) and input vector (x)\n",
    "def predict(weights, inputs):\n",
    "    # Ensure inputs is a 2D array with one row if it's a 1D array\n",
    "    inputs = np.atleast_2d(inputs)\n",
    "    bias = np.ones((inputs.shape[0], 1))\n",
    "    inputs = np.hstack((inputs, bias))\n",
    "    return 1 / (1 + np.exp(-np.dot(inputs, weights)))\n",
    "\n",
    "\n",
    "  # Assuming sigmoid activation\n",
    "\n",
    "\n",
    "# get accuracy, confusion matrices, rate and optimized weight vector\n",
    "\n",
    "#dataset-data the model is trained on\n",
    "#split-fraction of the dataset that is being used for training\n",
    "#error-the fucntion tha will keep updating the weights until the error becomes less than this threshold\n",
    "#alpha-the learning rate for the perceptron training\n",
    "def train(dataset, split, error_threshold, alpha):\n",
    "        \n",
    "    #splitting into training and testing data\n",
    "    group0 = dataset[dataset.iloc[:, 2] == 0]\n",
    "    group1 = dataset[dataset.iloc[:, 2] == 1]\n",
    "    sample_size = int(min(len(group0), len(group1)) * split)\n",
    "    sampled_subset0 = group0.sample(n = sample_size, random_state = 42)\n",
    "    sampled_subset1 = group1.sample(n = sample_size, random_state = 42)\n",
    "    training = pd.concat([sampled_subset0, sampled_subset1])\n",
    "    testing = dataset.drop(training.index)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    #initialization stuff\n",
    "    limit = 5000\n",
    "    patterns = training.shape[0]\n",
    "    w = [random.uniform(-0.5, 0.5) for _ in range(training.shape[1])]\n",
    "\n",
    "    batch_size = 32  # Size of mini-batches\n",
    "    \n",
    "    \n",
    "    #training \n",
    "    i = 0\n",
    "    error = patterns  # Initial error\n",
    "\n",
    "    while i < limit and error > error_threshold:\n",
    "        training = shuffle(training)  # Shuffle the data at the beginning of each epoch\n",
    "        for batch in range(0, patterns, batch_size):\n",
    "            X_batch = training.iloc[batch:batch + batch_size].drop(2, axis=1).values\n",
    "            y_batch = training.iloc[batch:batch + batch_size, 2].values\n",
    "\n",
    "\n",
    "\n",
    "            # Forward pass\n",
    "            scaled_X_batch = predict(w, X_batch)\n",
    "\n",
    "            # Compute the gradient\n",
    "            # Append a column of ones to X_batch for the bias term\n",
    "            X_batch_bias = np.hstack((X_batch, np.ones((X_batch.shape[0], 1))))\n",
    "\n",
    "            # Compute the gradient\n",
    "            delta_w = alpha * np.dot((y_batch - scaled_X_batch), X_batch_bias) / batch_size\n",
    "\n",
    "\n",
    "            # Update weights\n",
    "            w += delta_w\n",
    "\n",
    "        # Compute the error at the end of each epoch\n",
    "        error = np.sum((training.iloc[:, 2].values - predict(w, training.drop(2, axis=1).values)) ** 2)\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Total error in training is: {error}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #testing\n",
    "    true_positive = false_positive = true_negative = false_negative = 0\n",
    "    for row in range(testing.shape[0]):\n",
    "        x = testing.iloc[row, :-1].values  # Exclude the last value (the label)\n",
    "        predicted = 1 if predict(w, x) >= 0.5 else 0\n",
    "        true = testing.iloc[row, 2]\n",
    "        \n",
    "        if predicted == 1 and true == 1:\n",
    "            true_positive += 1\n",
    "        elif predicted == 1 and true == 0:\n",
    "            false_positive += 1\n",
    "        elif predicted == 0 and true == 1:\n",
    "            false_negative += 1\n",
    "        else:\n",
    "            true_negative += 1\n",
    "    \n",
    "    accuracy = (true_positive + true_negative) / testing.shape[0]\n",
    "    denominator_tp_fp = true_positive + false_positive\n",
    "    true_positive_rate = (true_positive / denominator_tp_fp * 100) if denominator_tp_fp != 0 else 0\n",
    "    false_positive_rate = (false_positive / denominator_tp_fp * 100) if denominator_tp_fp != 0 else 0\n",
    "    denominator_tn_fn = true_negative + false_negative\n",
    "    true_negative_rate = (true_negative / denominator_tn_fn * 100) if denominator_tn_fn != 0 else 0\n",
    "    false_negative_rate = (false_negative / denominator_tn_fn * 100) if denominator_tn_fn != 0 else 0\n",
    "    \n",
    "    \n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    print(\"\\t\\tTrue yes\\t\\tTrue no\")\n",
    "    print(f\"Predicted yes\\t{true_positive} ({true_positive_rate}%)\\t\\t{false_positive} ({false_positive_rate}%)\")\n",
    "    print(f\"Predicted no\\t{false_negative} ({false_negative_rate}%)\\t\\t{true_negative} ({true_negative_rate}%)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Plotting\n",
    "    # plt.scatter(training.iloc[:, 0], training.iloc[:, 1], c=training.iloc[:, 2], cmap='viridis', label='Training Data')\n",
    "    # plt.scatter(testing.iloc[:, 0], testing.iloc[:, 1], c=testing.iloc[:, 2], cmap='viridis', marker='x', label='Testing Data')\n",
    "\n",
    "    # #setting x and y variables\n",
    "    # # x = np.linspace(start=min(dataset.iloc[:, 0]), stop=max(dataset.iloc[:, 0]), num=100)\n",
    "    # x = np.linspace(training.iloc[:, 0].min(), training.iloc[:, 0].max(), 100)  # 100 points between min and max x values y_vals = - (w[0] / w[1]) * x_vals - (w[2] / w[1]) plt.plot(x_vals, y_vals, 'k--')  # 'k--' specifies a black dashed line\n",
    "    # y = (-w[0] * x - w[2]) / w[1]\n",
    "    # plt.plot(x, y, '-r', label='Decision Boundary')\n",
    "\n",
    "\n",
    "\n",
    "    # plt.plot(np.linspace(-10,10, 100))\n",
    "\n",
    "    # #labeling axis\n",
    "    # plt.xlabel('Normalized Cost(USD)')\n",
    "    # plt.ylabel('Normalized Weight(pounds)')\n",
    "    # plt.legend(loc='best')\n",
    "    # plt.show()\n",
    "    \n",
    "    # w = w / np.linalg.norm(w)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(training.iloc[:, 0], training.iloc[:, 1], c=training.iloc[:, 2], cmap='viridis', label='Training Data')\n",
    "    plt.scatter(testing.iloc[:, 0], testing.iloc[:, 1], c=testing.iloc[:, 2], cmap='viridis', marker='x', label='Testing Data')\n",
    "\n",
    "    x = np.linspace(0, 1, 400)\n",
    "    y = -(w[0]/w[1]) * x - (w[2]/w[1])   \n",
    "\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.show()\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the datasets\n",
    "a = pd.read_csv(\"groupA.csv\", header = None)\n",
    "b = pd.read_csv(\"groupB.csv\", header = None)\n",
    "c = pd.read_csv(\"groupC.csv\", header = None)\n",
    "patterns = len(a)\n",
    "\n",
    "\n",
    "\n",
    "#convert to numeric\n",
    "for col in a.columns:\n",
    "    a[col] = a[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    b[col] = b[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    c[col] = c[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    \n",
    "    \n",
    "\n",
    "#normalize the data\n",
    "norm(a)\n",
    "norm(b)\n",
    "norm(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hard activation\n",
    "# print(\"HARD ACTIVATION\")\n",
    "# print(\"---------------------------------------\")\n",
    "# print(\"\\nGroupA\")\n",
    "# print(train(a, 3/4, 0.00001, 0.1))\n",
    "# print(train(a, 1/4, 0.00001, 0.1))\n",
    "\n",
    "# print(\"\\nGroupB\")\n",
    "# print(train(b, 3/4, 40, 0.1))\n",
    "# print(train(b, 1/4, 40, 0.1))\n",
    "\n",
    "# print(\"\\nGroupC\")\n",
    "# print(train(c, 3/4, 700, 0.1)) #fix the train method\n",
    "# print(train(c, 1/4, 700, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFT ACTIVATION\n",
      "---------------------------------------\n",
      "\n",
      "GroupA\n"
     ]
    }
   ],
   "source": [
    "#soft activation\n",
    "print(\"SOFT ACTIVATION\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"\\nGroupA\")\n",
    "print(train(a, 3/4, 0.00001, 0.1)) #75\n",
    "print(train(a, 1/4, 0.00001, 0.1)) #25\n",
    "\n",
    "print(\"\\nGroupB\")\n",
    "print(train(b, 3/4, 40, 0.1))#75\n",
    "print(train(b, 1/4, 40, 0.1))#25\n",
    "\n",
    "print(\"\\nGroupC\")\n",
    "print(train(c, 3/4, 700, 0.1))#75\n",
    "print(train(c, 1/4, 700, 0.1))#25\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
