{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have accomodations to extend this assignment\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring norm function with the 2d array dataset\n",
    "def norm(dataset):\n",
    "    #looping though each column index except the last colums\n",
    "    for column_index in range(dataset.shape[1] - 1):\n",
    "        \n",
    "        #determining min and max values from the current columns to scale the data between [0,1]\n",
    "        max_val = dataset[column_index].max()\n",
    "        min_val = dataset[column_index].min()\n",
    "\n",
    "        #normalizing the column to fall within the range [0,1]\n",
    "        #this will modify the dataset in place allowing the original values to be replaced by the normalized values\n",
    "        dataset[column_index] = (dataset[column_index] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict function with 2 parmeters weight vector (w) and input vector (x)\n",
    "def predict(w, x):\n",
    "    #computing the dot product between weight and input vectors\n",
    "    scaled_x = np.dot(w, x)\n",
    "    if scaled_x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# get accuracy, confusion matrices, rate and optimized weight vector\n",
    "\n",
    "#dataset-data the model is trained on\n",
    "#split-fraction of the dataset that is being used for training\n",
    "#error-the fucntion tha will keep updating the weights until the error becomes less than this threshold\n",
    "#alpha-the learning rate for the perceptron training\n",
    "def train(dataset, split, error_threshold, alpha):\n",
    "        \n",
    "    #splitting into training and testing data\n",
    "    group0 = dataset[dataset.iloc[:, 2] == 0]\n",
    "    group1 = dataset[dataset.iloc[:, 2] == 1]\n",
    "    sample_size = int(min(len(group0), len(group1)) * split)\n",
    "    sampled_subset0 = group0.sample(n = sample_size, random_state = 42)\n",
    "    sampled_subset1 = group1.sample(n = sample_size, random_state = 42)\n",
    "    training = pd.concat([sampled_subset0, sampled_subset1])\n",
    "    testing = dataset.drop(training.index)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    #initialization stuff\n",
    "    limit = 5000\n",
    "    patterns = training.shape[0]\n",
    "    w = [random.uniform(-0.5, 0.5) for _ in range(training.shape[1])]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training \n",
    "    i = 0\n",
    "    error = patterns\n",
    "    #while i is less than 5000 or it is not accurate enough\n",
    "    while i < limit and error > error_threshold:\n",
    "        \n",
    "        for row in range(patterns):\n",
    "            x = training.iloc[row].values\n",
    "            scaled_x = predict(w, x)\n",
    "                \n",
    "            delta_weight = alpha * (x[2] - scaled_x)\n",
    "            delta_weighted_x = x * delta_weight\n",
    "            w = np.array(w) + np.array(delta_weighted_x)\n",
    "            \n",
    "            \n",
    "        #make sure w is correct\n",
    "        #the error in this iteration of the neuron\n",
    "        error = 0\n",
    "        for row in range(patterns):\n",
    "            x = training.iloc[row].values\n",
    "            \n",
    "            #for total error sum[(out - desired)^2]. (-1)^2 = 1 and 1^2 = 1 so += 1 when they're different\n",
    "            if x[2] != scaled_x:\n",
    "                error += 1\n",
    "        \n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    print(f\"Total error in training is: {error}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #testing\n",
    "    true_positive = false_positive = true_negative = false_negative = 0\n",
    "    for row in range(testing.shape[0]):\n",
    "        x = testing.iloc[row].values\n",
    "        predicted = predict(w, x)\n",
    "        true = x[2]\n",
    "        \n",
    "        if predicted == 1 and true == 1:\n",
    "            true_positive += 1\n",
    "        elif predicted == 1 and true == 0:\n",
    "            false_positive += 1\n",
    "        elif predicted == 0 and true == 1:\n",
    "            false_negative += 1\n",
    "        else:\n",
    "            true_negative += 1\n",
    "    \n",
    "    accuracy = (true_positive + true_negative) / testing.shape[0]\n",
    "    true_positive_rate = true_positive / (true_positive + false_positive) * 100\n",
    "    false_positive_rate = false_positive / (true_positive + false_positive) * 100\n",
    "    true_negative_rate = true_negative / (true_negative + false_negative) * 100\n",
    "    false_negative_rate = false_negative / (true_negative + false_negative) * 100\n",
    "    \n",
    "    \n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    print(\"\\t\\tTrue yes\\t\\tTrue no\")\n",
    "    print(f\"Predicted yes\\t{true_positive} ({true_positive_rate}%)\\t\\t{false_positive} ({false_positive_rate}%)\")\n",
    "    print(f\"Predicted no\\t{false_negative} ({false_negative_rate}%)\\t\\t{true_negative} ({true_negative_rate}%)\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Plotting\n",
    "    # plt.scatter(training.iloc[:, 0], training.iloc[:, 1], c=training.iloc[:, 2], cmap='viridis', label='Training Data')\n",
    "    # plt.scatter(testing.iloc[:, 0], testing.iloc[:, 1], c=testing.iloc[:, 2], cmap='viridis', marker='x', label='Testing Data')\n",
    "\n",
    "    # #setting x and y variables\n",
    "    # # x = np.linspace(start=min(dataset.iloc[:, 0]), stop=max(dataset.iloc[:, 0]), num=100)\n",
    "    # x = np.linspace(training.iloc[:, 0].min(), training.iloc[:, 0].max(), 100)  # 100 points between min and max x values y_vals = - (w[0] / w[1]) * x_vals - (w[2] / w[1]) plt.plot(x_vals, y_vals, 'k--')  # 'k--' specifies a black dashed line\n",
    "    # y = (-w[0] * x - w[2]) / w[1]\n",
    "    # plt.plot(x, y, '-r', label='Decision Boundary')\n",
    "\n",
    "\n",
    "\n",
    "    # plt.plot(np.linspace(-10,10, 100))\n",
    "\n",
    "    # #labeling axis\n",
    "    # plt.xlabel('Normalized Cost(USD)')\n",
    "    # plt.ylabel('Normalized Weight(pounds)')\n",
    "    # plt.legend(loc='best')\n",
    "    # plt.show()\n",
    "    \n",
    "    # w = w / np.linalg.norm(w)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(training.iloc[:, 0], training.iloc[:, 1], c=training.iloc[:, 2], cmap='viridis', label='Training Data')\n",
    "    plt.scatter(testing.iloc[:, 0], testing.iloc[:, 1], c=testing.iloc[:, 2], cmap='viridis', marker='x', label='Testing Data')\n",
    "\n",
    "    x = np.linspace(0, 1, 400)\n",
    "    y = -(w[0]/w[1]) * x - (w[2]/w[1])   \n",
    "\n",
    "    plt.plot(x, y, '-r')\n",
    "    plt.show()\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the datasets\n",
    "a = pd.read_csv(\"groupA.csv\", header = None)\n",
    "b = pd.read_csv(\"groupB.csv\", header = None)\n",
    "c = pd.read_csv(\"groupC.csv\", header = None)\n",
    "\n",
    "\n",
    "\n",
    "#convert to numeric\n",
    "for col in a.columns:\n",
    "    a[col] = a[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    b[col] = b[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    c[col] = c[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    \n",
    "    \n",
    "\n",
    "#normalize the data\n",
    "norm(a)\n",
    "norm(b)\n",
    "norm(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hard activation\n",
    "# print(\"HARD ACTIVATION\")\n",
    "# print(\"---------------------------------------\")\n",
    "# print(\"\\nGroupA\")\n",
    "# print(train(a, 3/4, 0.00001, 0.1))\n",
    "# print(train(a, 1/4, 0.00001, 0.1))\n",
    "\n",
    "# print(\"\\nGroupB\")\n",
    "# print(train(b, 3/4, 40, 0.1))\n",
    "# print(train(b, 1/4, 40, 0.1))\n",
    "\n",
    "# print(\"\\nGroupC\")\n",
    "# print(train(c, 3/4, 700, 0.1)) #fix the train method\n",
    "# print(train(c, 1/4, 700, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFT ACTIVATION\n",
      "---------------------------------------\n",
      "\n",
      "GroupA\n"
     ]
    }
   ],
   "source": [
    "#soft activation\n",
    "print(\"SOFT ACTIVATION\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"\\nGroupA\")\n",
    "print(train(a, 3/4, 0.00001, 0.1)) #75\n",
    "print(train(a, 1/4, 0.00001, 0.1)) #25\n",
    "\n",
    "print(\"\\nGroupB\")\n",
    "print(train(b, 3/4, 40, 0.1))#75\n",
    "print(train(b, 1/4, 40, 0.1))#25\n",
    "\n",
    "print(\"\\nGroupC\")\n",
    "print(train(c, 3/4, 700, 0.1))#75\n",
    "print(train(c, 1/4, 700, 0.1))#25\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
