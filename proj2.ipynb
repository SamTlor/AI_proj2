{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(dataset):\n",
    "    for column_index in range(dataset.shape[1] - 1):\n",
    "        max_val = dataset[column_index].max()\n",
    "        min_val = dataset[column_index].min()\n",
    "\n",
    "        dataset[column_index] = (dataset[column_index] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    scaled_x = np.dot(w, x)\n",
    "    if scaled_x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# get accuracy, confusion matrices and rates\n",
    "def train(dataset, split, error_threshold, alpha):\n",
    "        \n",
    "    #splitting into training and testing data\n",
    "    group0 = dataset[dataset.iloc[:, 2] == 0]\n",
    "    group1 = dataset[dataset.iloc[:, 2] == 1]\n",
    "    sample_size = int(min(len(group0), len(group1)) * split)\n",
    "    sampled_subset0 = group0.sample(n = sample_size, random_state = 42)\n",
    "    sampled_subset1 = group1.sample(n = sample_size, random_state = 42)\n",
    "    training = pd.concat([sampled_subset0, sampled_subset1])\n",
    "    testing = dataset.drop(training.index)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    #initialization stuff\n",
    "    limit = 5000\n",
    "    patterns = training.shape[0]\n",
    "    w = [random.uniform(-0.5, 0.5) for _ in range(training.shape[1])]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training \n",
    "    i = 0\n",
    "    error = patterns\n",
    "    #while i is less than 5000 or it is not accurate enough\n",
    "    while i < limit or error > error_threshold:\n",
    "        #the error in this iteration of the neuron\n",
    "        error = 0\n",
    "        \n",
    "        for row in range(patterns):\n",
    "            x = training.iloc[row].values\n",
    "            scaled_x = predict(w, x)\n",
    "                \n",
    "            #for total error sum[(out - desired)^2]. (-1)^2 = 1 and 1^2 = 1 so += 1 when they're different\n",
    "            if x[2] != scaled_x:\n",
    "                error += 1\n",
    "                \n",
    "            delta_weight = alpha * (x[2] - scaled_x)\n",
    "            delta_weighted_x = x * delta_weight\n",
    "            w = np.array(w) + np.array(delta_weighted_x)\n",
    "        i += 1\n",
    "    \n",
    "    print(i)\n",
    "    print(f\"Total error in training is: {error}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #testing\n",
    "    true_positive = false_positive = true_negative = false_negative = 0\n",
    "    for row in range(testing.shape[0]):\n",
    "        x = testing.iloc[row].values\n",
    "        predicted = predict(w, x)\n",
    "        true = x[2]\n",
    "        \n",
    "        if predicted == 1 and true == 1:\n",
    "            true_positive += 1\n",
    "        elif predicted == 1 and true == 0:\n",
    "            false_positive += 1\n",
    "        elif predicted == 0 and true == 1:\n",
    "            false_negative += 1\n",
    "        else:\n",
    "            true_negative += 1\n",
    "    \n",
    "    accuracy = (true_positive + true_negative) / testing.shape[0]\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the datasets\n",
    "a = pd.read_csv(\"groupA.csv\", header = None)\n",
    "b = pd.read_csv(\"groupB.csv\", header = None)\n",
    "c = pd.read_csv(\"groupC.csv\", header = None)\n",
    "\n",
    "\n",
    "\n",
    "#convert to numeric\n",
    "for col in a.columns:\n",
    "    a[col] = a[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    b[col] = b[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    c[col] = c[col].apply(pd.to_numeric, errors = 'coerce')\n",
    "    \n",
    "    \n",
    "\n",
    "#normalize the data\n",
    "norm(a)\n",
    "norm(b)\n",
    "norm(c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#soft activation\n",
    "# print(train(a, 3/4, 0.00001, 0.01))\n",
    "print(train(a, 1/4, 0.00001, 0.01))\n",
    "\n",
    "# print(train(b, 3/4, 40, 0.01))\n",
    "# print(train(b, 1/4, 40, 0.01))\n",
    "\n",
    "# print(train(c, 3/4, 700, 0.01))\n",
    "# print(train(c, 1/4, 700, 0.01))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #hard activation\n",
    "# print(train(a, 3/4, 0.00001, 0.1))\n",
    "# print(train(a, 1/4, 0.00001, 0.1))\n",
    "\n",
    "# print(train(b, 3/4, 40, 0.1))\n",
    "# print(train(b, 1/4, 40, 0.1))\n",
    "\n",
    "# print(train(c, 3/4, 700, 0.1))\n",
    "# print(train(c, 1/4, 700, 0.1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# matplotlib stuff here? store the values of the weight, plot the data, draw the division line\n",
    "# train() function can print the accuracy and confusion matrices\n",
    "# maybe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
